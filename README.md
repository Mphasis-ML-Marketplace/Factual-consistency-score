# Factual-Consistency-Evaluation-Metric
Estimating GenAI(LLM) Model Reliability Through Evaluation of Factual Consistency.

## Product Overview
In today's era of GenAI reliance, the lack of an evaluation metric for factual consistency poses a challenge due to hallucination. Our solution addresses this by offering a unique method to verify the accuracy of LLM model-generated responses, leveraging Claude from Anthropic as the foundation model. We employ a multi-step process, including question generation, validation, and comparison against reference answers, to ensure factual consistency. Our approach is versatile; for instance, in customer service, it checks if representatives (AI chatbot/human) provide correct information from a knowledge base. It extends beyond AI responses, working in scenarios where accuracy is vital, such as educational platforms or content moderation systems. This broad applicability underscores our solution's effectiveness in maintaining factual consistency across various domains. The offering requires an AWS bedrock anthropic Claude Instant model subscription.

## Product Highlight
* Our GenAI solution tackles the challenge of assessing factual consistency in LLM model-generated responses. It involves extracting noun chunks, generating fact-based questions, and comparing LLM responses against reference answers. This comprehensive approach ensures accurate evaluation outcomes, empowering users to make informed decisions regarding GenAI model suitability and performance.
* Our proposed LLM modal evaluation metric improves model assessment by providing a robust means of evaluating factual consistency. By filtering responses based on this metric, users can identify high-quality outputs, enhancing overall model efficacy and utility across diverse applications.
* Mphasis DeepInsights is a cloud-based cognitive computing platform that offers data extraction & predictive analytics capabilities. Need Customized Deep learning and Machine Learning Solutions? Get in Touch!

